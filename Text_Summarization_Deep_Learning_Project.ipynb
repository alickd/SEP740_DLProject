{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alickd/SEP740_DLProject/blob/main/Text_Summarization_Deep_Learning_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK-ty_VzQrQ1"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Text Summarizer "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dylan Alick & Jonathan Wong "
      ],
      "metadata": {
        "id": "LOzPq0ErYr26"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hv-GqcSOrxk"
      },
      "outputs": [],
      "source": [
        "#Install transformers for pre-trained Hugging Face Libraries\n",
        "!pip install transformers\n",
        "!pip install sumy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRjW1VU-2vLp"
      },
      "outputs": [],
      "source": [
        "# Moutning Google Drive to be \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gqkg9SxOFZoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f238139c-7eb3-49ba-9d6b-e7cc52b68338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import re\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras import backend as K\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import gensim\n",
        "from gensim.summarization import summarize\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.utils.data_utils import pad_sequences\n",
        "import keras_preprocessing\n",
        "from keras.preprocessing import text,sequence\n",
        "from keras.models import Model\n",
        "from keras.layers import Input,Dense,Embedding,LSTM,Dropout, RepeatVector, concatenate, TimeDistributed\n",
        "from tensorflow.keras.layers import Attention\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "GkB_P4eoTxRQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "79368db0-9931-4713-fa48-3a8322dbae3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Summary                                               Text\n",
              "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
              "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
              "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
              "4            Great taffy  Great taffy at a great price.  There was a wid..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-71d1045e-e5c1-4540-a435-7a87ebb52249\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71d1045e-e5c1-4540-a435-7a87ebb52249')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-71d1045e-e5c1-4540-a435-7a87ebb52249 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-71d1045e-e5c1-4540-a435-7a87ebb52249');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "#Import and read dataset\n",
        "path  = \"/content/drive/MyDrive/Deep Learning Project/Reviews.xlsx\"\n",
        "#Reducing data size due to computational limitations, dataset size = 100k\n",
        "df_reviews = pd.read_csv(path,nrows=100000)\n",
        "#Drop columns not needed for model\n",
        "df_reviews.drop(columns=['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator','Score','Time'],inplace=True,axis=1)\n",
        "df_reviews.dropna(axis=0,inplace=True)\n",
        "df_reviews.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EJ-5kOo5cWl"
      },
      "source": [
        "LSA Machine Learning Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vmKfOha_LLQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8f6412-570c-405d-ee52-2c023abd39de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
            "---------------------------------------------------------------------\n",
            "Summary\n",
            "It is a light, pillowy citrus gelatin with nuts - in this case Filberts.\n"
          ]
        }
      ],
      "source": [
        "# Import the summarizer \n",
        "import sumy\n",
        "from sumy.summarizers.lsa import LsaSummarizer \n",
        "\n",
        "# Text to summarize \n",
        "lst_summaries = []\n",
        "lsa_summary = []\n",
        "\n",
        "lst_summaries=[df_reviews['Text'][1],df_reviews['Text'][2]]\n",
        "\n",
        "# Parsing the text string using PlaintextParser \n",
        "from sumy.nlp.tokenizers import Tokenizer \n",
        "from sumy.parsers.plaintext import PlaintextParser \n",
        "\n",
        "parser=PlaintextParser.from_string(lst_summaries,Tokenizer('english')) \n",
        "\n",
        "# Creating the summarizer \n",
        "lsa_summarizer=LsaSummarizer()\n",
        "lsa_summary= lsa_summarizer(parser.document,1)\n",
        "\n",
        "# Printing the summary \n",
        "print(lst_summaries[0])\n",
        "print(\"---------------------------------------------------------------------\")\n",
        "print(\"Summary\")\n",
        "for sentence in lsa_summary: \n",
        "    print(sentence) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN3YpmHXR6cS"
      },
      "source": [
        "Pre-Processing Text Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DGpWI_W8D-9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f109f42-d863-4aab-a64b-ef2b70c52f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Pre-Processing\n",
            "Review  1\n",
            "I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
            "Summary\n",
            "Good Quality Dog Food\n",
            "Review  2\n",
            "Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
            "Summary\n",
            "Not as Advertised\n",
            "Review  3\n",
            "This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
            "Summary\n",
            "\"Delight\" says it all\n"
          ]
        }
      ],
      "source": [
        "print(\"Before Pre-Processing\")\n",
        "#Printing first three rows of original reviews and summary\n",
        "for i in range(3):\n",
        "  print(\"Review \", i+1)\n",
        "  print(df_reviews['Text'][i])\n",
        "  print(\"Summary\")\n",
        "  print(df_reviews['Summary'][i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DgA93zzmSAWK"
      },
      "outputs": [],
      "source": [
        "#Function created to clean text as part of preprocessing data - includes removing numbers, punctuation, special characters, and stopwords\n",
        "def clean_text(text):\n",
        "  #Removing numbers\n",
        "  text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "  #Convert text into lower case\n",
        "  text = text.lower()\n",
        "  #Remove URL links\n",
        "  text = re.sub(r'http\\S+',' ',text)\n",
        "  #Removing extra spaces/lines\n",
        "  text = re.sub(' +',' ',text)\n",
        "  #Removing punctuation and characters\n",
        "  text = re.sub('[^\\w\\s]','',text)\n",
        "  #Removing stopwords\n",
        "  text = remove_stopwords(text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "JEZqwCKgSX7l"
      },
      "outputs": [],
      "source": [
        "#Pre-Processing Text Data\n",
        "df_reviews[\"Summary\"] = df_reviews[\"Summary\"].apply(clean_text)\n",
        "df_reviews[\"Text\"] = df_reviews[\"Text\"].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For Seq2Seq model, adding a start and end token for processing\n",
        "df_reviews['Text'] = df_reviews['Text'].apply(lambda x : 'start '+ x + ' end')"
      ],
      "metadata": {
        "id": "lZyumVdDjs73"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "jUOz_8BRH3AP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e885471-df90-4c5a-8b00-c5076b6e1c31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Pre-Processing\n",
            "Review  1\n",
            "bought vitality canned dog food products good quality product looks like stew processed meat smells better labrador finicky appreciates product better\n",
            "Summary\n",
            "good quality dog food\n",
            "Review  2\n",
            "product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo\n",
            "Summary\n",
            "advertised\n",
            "Review  3\n",
            "confection centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story c s lewis lion witch wardrobe treat seduces edmund selling brother sisters witch\n",
            "Summary\n",
            "delight says\n"
          ]
        }
      ],
      "source": [
        "print(\"After Pre-Processing\")\n",
        "for i in range(3):\n",
        "  print(\"Review \", i+1)\n",
        "  print(df_reviews['Text'][i])\n",
        "  print(\"Summary\")\n",
        "  print(df_reviews['Summary'][i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfEuVx6oa9r5"
      },
      "source": [
        "Visualize the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDFPSRYloDHl"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import collections\n",
        "\n",
        "lemmatized_tokens = list(df_reviews[\"Summary\"])\n",
        "token_list = list(itertools.chain(lemmatized_tokens))\n",
        "\n",
        "#Frequency of Words Plot\n",
        "counts_no=collections.Counter(token_list)\n",
        "clean_words = pd.DataFrame(counts_no.most_common(30),columns=['text','count'])\n",
        "fig,ax = plt.subplots(figsize=(8,8))\n",
        "clean_words.sort_values(by='count').plot.barh(x='text',y='count',ax=ax,color='green')\n",
        "ax.set_title(\"Frequency of Words\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "3FXeu-VNao3g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "d05d7430-b70d-49b8-b8ef-60055ba86fb5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#Plotting distrubution of word count for reviews vs summary\n",
        "text_word_count = []\n",
        "summary_word_count = []\n",
        "\n",
        "#Populate the lists with sentence lengths\n",
        "for i in df_reviews['Text']:\n",
        "      text_word_count.append(len(i.split()))\n",
        "\n",
        "for i in df_reviews['Summary']:\n",
        "      summary_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'Text':text_word_count, 'Summary':summary_word_count})\n",
        "\n",
        "length_df.hist()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN_TEXT = 80\n",
        "MAX_LEN_SUM = 15"
      ],
      "metadata": {
        "id": "VOi8OfbXcFFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7Z6Tzy71mjg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_text = list(df_reviews['Text'].values)\n",
        "y_summary = list(df_reviews['Summary'].values)\n",
        "reviews = df_reviews['Text']\n",
        "labels = df_reviews['Summary']\n",
        "\n",
        "train_reviews, val_reviews, train_labels, val_labels = train_test_split(reviews, labels, test_size=.3)\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X_text,y_summary,test_size=0.2,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oeHLE7ZcYEp"
      },
      "outputs": [],
      "source": [
        "#Tokenization\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "\n",
        "x_tokenizer = Tokenizer()\n",
        "y_tokenizer = Tokenizer()\n",
        "\n",
        "\n",
        "x_tokenizer.fit_on_texts(list(X_train))\n",
        "y_tokenizer.fit_on_texts(list(Y_train))\n",
        "\n",
        "#Tokenize Train - convert text sequences to integer sequences\n",
        "x_train_token = x_tokenizer.texts_to_sequences(X_train)\n",
        "y_train_token = y_tokenizer.texts_to_sequences(Y_train)\n",
        "\n",
        "#Padding zeros up to the maximum length of text so all is same\n",
        "x_train = pad_sequences(x_train_token, maxlen = MAX_LEN_TEXT, padding = 'post')\n",
        "y_train = pad_sequences(y_train_token, maxlen = MAX_LEN_TEXT, padding = 'post')\n",
        "\n",
        "#Tokenize Test\n",
        "x_test_token = x_tokenizer.texts_to_sequences(X_test)\n",
        "y_test_token = y_tokenizer.texts_to_sequences(Y_test)\n",
        "\n",
        "#Padding zeros up to the maximum length of text so all is same\n",
        "x_test = pad_sequences(x_test_token, maxlen = MAX_LEN_TEXT)\n",
        "y_test = pad_sequences(y_test_token, maxlen = MAX_LEN_TEXT)\n",
        "\n",
        "x_voc_size = len(x_tokenizer.word_index)+1\n",
        "y_voc_size= len(y_tokenizer.word_index)+1\n",
        "\n",
        "print(X_train[1]) #text data\n",
        "print(Y_train[1])\n",
        "print(x_train.shape) #text data in tokens\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "latent_dim = 500"
      ],
      "metadata": {
        "id": "51GJw9A_fBN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "encoder_inputs = Input(shape=(MAX_LEN_TEXT,))\n",
        "enc_emb = Embedding(x_voc_size, latent_dim, trainable=True)(encoder_inputs)\n",
        "\n",
        "# LSTM 1\n",
        "encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "# LSTM 2\n",
        "encoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm1(encoder_output1)\n",
        "\n",
        "# LSTM 3\n",
        "encoder_lstm3 = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm1(encoder_output2)\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(y_voc_size, latent_dim, trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# LSTM using encoder_states as initial state\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_ouputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "# Attention layer\n",
        "#attn_layer = Attention(name='attention_layer')\n",
        "#attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "# Dense Layer\n",
        "decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_ouputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs,name='Seq2Seq')\n"
      ],
      "metadata": {
        "id": "7O84AvVFfHfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "ElMVeYW9ff15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',metrics='accuracy')"
      ],
      "metadata": {
        "id": "iSkCw3infs9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)"
      ],
      "metadata": {
        "id": "IKueB2iUfuH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit([x_train, y_train[:,:-1]], y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:,1:],\n",
        "                  epochs=5, batch_size=512, verbose =2,\n",
        "                  validation_data=([x_test, y_test[:,:-1]], y_test.reshape(y_test.shape[0], y_test.shape[1], 1)[:,1:]))"
      ],
      "metadata": {
        "id": "NDh4WNUXgCms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5t622fjviCC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9ben7zo_jSsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_target_word_index = y_tokenizer.index_word \n",
        "reverse_source_word_index = x_tokenizer.index_word \n",
        "target_word_index = y_tokenizer.word_index"
      ],
      "metadata": {
        "id": "Qg5gtw2Fi7Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder inference\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder inference\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(MAX_LEN_TEXT, latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "# Attention inference ??\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "[decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
        "[decoder_outputs2] + [state_h2, state_c2])"
      ],
      "metadata": {
        "id": "YZBpNMX-iXpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # Chose the 'start' word as the first word of the target sequence\n",
        "    target_seq[0, 0] = target_word_index[\"start\"]\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        \n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "\n",
        "        if(sampled_token!='end'):\n",
        "          decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'end' or len(decoded_sentence.split()) >= (MAX_LEN_SUMMARY-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c"
      ],
      "metadata": {
        "id": "bbQU16lyie0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if ((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
        "        newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if (i!=0):\n",
        "        newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString"
      ],
      "metadata": {
        "id": "MBa9Zru7imna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print(\"Review:\",seq2text(x_test[i]))\n",
        "  print(\"Original summary:\",seq2summary(y_test[i]))\n",
        "  print(\"Predicted summary:\",decode_sequence(x_test[i].reshape(1, MAX_LEN_TEXT)))\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "5eG6lzsNitTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNmE8hsDOYI8"
      },
      "source": [
        "Hugging Face Transformers - Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pafhiKdmaH2c"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from transformers import TFDistilBertForSequenceClassification\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from transformers import TFTrainer, TFTrainingArguments\n",
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "mdl = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "#model = TFAutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-mSsPoYz3wH"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(X_train, truncation = True, padding = True)\n",
        "val_encodings = tokenizer(Y_train, truncation= True, padding = True)\n",
        "test_encodings = tokenizer(Y_test, truncation = True, padding = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSYka1UW0761"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    X_train\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    Y_test\n",
        "))"
      ],
      "metadata": {
        "id": "tuz2dipdSOEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=mdl,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    #data_collator=data_collator,\n",
        ")\n"
      ],
      "metadata": {
        "id": "lELofbA_ncc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "3s86rErBn-QE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "DYS5X9ntw8kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_train_dataset = tokenized_dataset['train'].to_tf_dataset(\n",
        "    columns=['attention_mask', 'input_ids'],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_validation_dataset = tokenized_dataset[\"train\"].to_tf_dataset(\n",
        "    columns=['attention_mask', 'input_ids'],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "bUPGiBSxocsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import create_optimizer\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 16\n",
        "num_epochs = 5\n",
        "batches_per_epoch = len(tokenized_dataset[\"train\"]) // batch_size\n",
        "total_train_steps = int(batches_per_epoch * num_epochs)\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2e-5, \n",
        "    num_warmup_steps=0, \n",
        "    num_train_steps=total_train_steps\n",
        ")"
      ],
      "metadata": {
        "id": "jtWqNfFlorex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZZe-u8IzUQG"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lst_summaries=[]\n",
        "for i in range (X_train):\n",
        "  lst_summaries = X_train[i]"
      ],
      "metadata": {
        "id": "pHme5kGF4J1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lst_summaries)"
      ],
      "metadata": {
        "id": "L1DxgTOc429m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = bart"
      ],
      "metadata": {
        "id": "6oIrt58O4epm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlvTAebnQasr"
      },
      "outputs": [],
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgWf3QDQ-CvB"
      },
      "outputs": [],
      "source": [
        "sum_generated = summarizer(df_reviews['Text'][1])\n",
        "print(sum_generated)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "PxnWDkyiIOxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"csv\", data_files=\"/content/drive/MyDrive/Deep Learning Project/Reviews.csv\")"
      ],
      "metadata": {
        "id": "6B1B9XAUHvoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][1]"
      ],
      "metadata": {
        "id": "xw04n2XmpFWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "r78YT7ZrpdFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"Text\"], truncation=True)"
      ],
      "metadata": {
        "id": "YEX7M1O5phFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "h5R3DMbXpkCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "gZDeCAqtqgpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "mod = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
      ],
      "metadata": {
        "id": "E_-4TVppqhuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "yBB6QCUiq3-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_train_dataset = tokenized_dataset['train'].to_tf_dataset(\n",
        "    columns=['attention_mask', 'input_ids','Summary'],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_validation_dataset = tokenized_dataset[\"train\"].to_tf_dataset(\n",
        "    columns=['attention_mask', 'input_ids','Time'],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "Pmtkx2XyrBTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import create_optimizer\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 16\n",
        "num_epochs = 5\n",
        "batches_per_epoch = len(tokenized_dataset[\"train\"]) // batch_size\n",
        "total_train_steps = int(batches_per_epoch * num_epochs)\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2e-5, \n",
        "    num_warmup_steps=0, \n",
        "    num_train_steps=total_train_steps\n",
        ")"
      ],
      "metadata": {
        "id": "WQC2Y67Rr_7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "wuk = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
      ],
      "metadata": {
        "id": "o4ZuRNLwsEkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "wuk.compile(optimizer=optimizer)"
      ],
      "metadata": {
        "id": "SEU24YTssLiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wuk.fit(\n",
        "    tf_train_dataset,\n",
        "    validation_data=tf_validation_dataset,\n",
        "    epochs=5,\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "id": "CPTFL3mQsSzJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}