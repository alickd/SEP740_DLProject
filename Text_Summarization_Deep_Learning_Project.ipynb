{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alickd/SEP740_DLProject/blob/main/Text_Summarization_Deep_Learning_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK-ty_VzQrQ1"
      },
      "source": [
        "Text Summarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hv-GqcSOrxk"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sumy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRjW1VU-2vLp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqkg9SxOFZoJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import re\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import gensim\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.utils.data_utils import pad_sequences\n",
        "import keras_preprocessing\n",
        "from keras.preprocessing import text,sequence\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.layers import Input,Dense,Embedding,LSTM,Dropout, RepeatVector, concatenate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkB_P4eoTxRQ"
      },
      "outputs": [],
      "source": [
        "#Import and read dataset\n",
        "path  = \"/content/drive/MyDrive/Deep Learning Project/Reviews.xlsx\"\n",
        "df_reviews = pd.read_csv(path,nrows=100)\n",
        "df_reviews.drop(columns=['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator','Score','Time'],inplace=True,axis=1)\n",
        "df_reviews.dropna(axis=0,inplace=True)\n",
        "df_reviews.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EJ-5kOo5cWl"
      },
      "source": [
        "LSA Machine Learning Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmKfOha_LLQe"
      },
      "outputs": [],
      "source": [
        "# Import the summarizer \n",
        "import sumy\n",
        "from sumy.summarizers.lsa import LsaSummarizer \n",
        "\n",
        "# Text to summarize \n",
        "original_text = df_reviews.Text[67] \n",
        "\n",
        "# Parsing the text string using PlaintextParser \n",
        "from sumy.nlp.tokenizers import Tokenizer \n",
        "from sumy.parsers.plaintext import PlaintextParser \n",
        "\n",
        "parser=PlaintextParser.from_string(original_text,Tokenizer('english')) \n",
        "\n",
        "# Creating the summarizer \n",
        "lsa_summarizer=LsaSummarizer() \n",
        "lsa_summary= lsa_summarizer(parser.document,3) \n",
        "\n",
        "# Printing the summary \n",
        "print(original_text)\n",
        "print(\"---------------------------------------------------------------------\")\n",
        "print(\"Summary\")\n",
        "for sentence in lsa_summary: \n",
        "    print(sentence) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN3YpmHXR6cS"
      },
      "source": [
        "Pre-Processing Text Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGpWI_W8D-9M"
      },
      "outputs": [],
      "source": [
        "print(\"Before Pre-Processing\")\n",
        "for i in range(3):\n",
        "  print(\"Review \", i+1)\n",
        "  print(df_reviews['Text'][i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "DgA93zzmSAWK"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "  #Removing numbers\n",
        "  text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "  #Convert text into lower case\n",
        "  text = text.lower()\n",
        "  #Remove URL links\n",
        "  text = re.sub(r'http\\S+',' ',text)\n",
        "  #Removing extra spaces/lines\n",
        "  text = re.sub(' +',' ',text)\n",
        "  #Removing punctuation\n",
        "  text = re.sub('[^\\w\\s]','',text)\n",
        "  #Removing stopwords\n",
        "  text = remove_stopwords(text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "JEZqwCKgSX7l"
      },
      "outputs": [],
      "source": [
        "#Pre-Processing Text Data\n",
        "df_reviews[\"Summary\"] = df_reviews[\"Summary\"].apply(clean_text)\n",
        "df_reviews[\"Text\"] = df_reviews[\"Text\"].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUOz_8BRH3AP"
      },
      "outputs": [],
      "source": [
        "print(\"After Pre-Processing\")\n",
        "for i in range(3):\n",
        "  print(\"Review \", i+1)\n",
        "  print(df_reviews['Text'][i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfEuVx6oa9r5"
      },
      "source": [
        "Visualize the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDFPSRYloDHl"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import collections\n",
        "\n",
        "lemmatized_tokens = list(df_reviews[\"Summary\"])\n",
        "token_list = list(itertools.chain(lemmatized_tokens))\n",
        "\n",
        "counts_no=collections.Counter(token_list)\n",
        "clean_words = pd.DataFrame(counts_no.most_common(30),columns=['text','count'])\n",
        "fig,ax = plt.subplots(figsize=(8,8))\n",
        "clean_words.sort_values(by='count').plot.barh(x='text',y='count',ax=ax,color='green')\n",
        "ax.set_title(\"Frequency of Words\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "3FXeu-VNao3g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "db7253ea-1025-4da0-d333-c82069d1a00b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "text_word_count = []\n",
        "summary_word_count = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in df_reviews['Text']:\n",
        "      text_word_count.append(len(i.split()))\n",
        "\n",
        "for i in df_reviews['Summary']:\n",
        "      summary_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'Text':text_word_count, 'Summary':summary_word_count})\n",
        "\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "L7Z6Tzy71mjg"
      },
      "outputs": [],
      "source": [
        "#X_text = df_reviews['Text'].values\n",
        "#y_summary = df_reviews['Summary'].values\n",
        "reviews = df_reviews['Text'].values\n",
        "labels = df_reviews['Summary'].values\n",
        "\n",
        "train_reviews, val_reviews, train_labels, val_labels = train_test_split(reviews, labels, test_size=.3)\n",
        "#X_train,Y_train,X_test,Y_test=train_test_split(X_text,y_summary,test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_dataset(X_text):\n",
        "    encoded = tokenizer(\n",
        "        X_text,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors='np',\n",
        "    )\n",
        "    return encoded.data\n",
        "\n",
        "# Need to convert to List[str] because the tokenizer expects List but not np.array\n",
        "tokenized_datasets = {\n",
        "    \"train\": tokenize_dataset(train_reviews.tolist()),\n",
        "    \"validation\": tokenize_dataset(val_reviews.tolist()),\n",
        "}"
      ],
      "metadata": {
        "id": "vIkuyvSwa3cB"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNmE8hsDOYI8"
      },
      "source": [
        "Hugging Face Transformers - Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pafhiKdmaH2c"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from transformers import TFDistilBertForSequenceClassification\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "#model = TFAutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='SparseCategoricalCrossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "RC6Nv8GUcJW4"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    tokenized_datasets['train'],\n",
        "    train_labels, \n",
        "    validation_data=(tokenized_datasets['validation'], val_labels),\n",
        "    batch_size=8,\n",
        "    epochs=3\n",
        ")"
      ],
      "metadata": {
        "id": "JbRFlhwWcQK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "yZZe-u8IzUQG"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-mSsPoYz3wH"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(X_train, truncation = True, padding = True)\n",
        "test_encodings = tokenizer(X_test, truncation = True, padding = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "OSYka1UW0761"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    Y_train\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    Y_test\n",
        "))"
      ],
      "metadata": {
        "id": "tuz2dipdSOEA"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlvTAebnQasr"
      },
      "outputs": [],
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", tokenizer = \"facebook/bart-large-cnn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgWf3QDQ-CvB"
      },
      "outputs": [],
      "source": [
        "sum_generated = summarizer(df_reviews['Text'][67])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6E-vNB69GkA"
      },
      "outputs": [],
      "source": [
        "input_ids = tokenizer.encode(\"summarize:\" + df_reviews[\"Text\"][1], return_tensors='tf',truncation=True, max_length=60)\n",
        "greedy_output = model.generate(input_ids,num_beams=4,\n",
        "                                    no_repeat_ngram_size=2,\n",
        "                                    min_length=30,\n",
        "                                    max_length=100,\n",
        "                                    early_stopping=True)\n",
        "tokenizer.decode(greedy_output[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhxDODcEuHJj"
      },
      "outputs": [],
      "source": [
        "df_reviews[\"Text\"][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "HewbLmhMQAUB"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msVddir1fRei"
      },
      "outputs": [],
      "source": [
        "model.fit(train_dataset,epochs=3,verbose=2,validation_data=test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukf66sVjc2eI"
      },
      "outputs": [],
      "source": [
        "#Tokenization\n",
        "max_vocab = 1000\n",
        "max_length = 200\n",
        "\n",
        "token = Tokenizer(num_words=max_vocab)\n",
        "token.fit_on_texts(X_train)\n",
        "token.fit_on_texts(Y_train)\n",
        "\n",
        "#Tokenize Train\n",
        "x_train_token = token.texts_to_sequences(X_train)\n",
        "x_train = pad_sequences(x_train_token, maxlen = max_length)\n",
        "\n",
        "y_train_token = token.texts_to_sequences(Y_train)\n",
        "y_train = pad_sequences(y_train_token, maxlen = max_length)\n",
        "\n",
        "#Tokenize Test\n",
        "x_test_token = token.texts_to_sequences(X_test)\n",
        "x_test = pad_sequences(x_test_token, maxlen = max_length)\n",
        "\n",
        "y_test_token = token.texts_to_sequences(Y_test)\n",
        "y_test = pad_sequences(y_train_token, maxlen = max_length)\n",
        "\n",
        "print(X_train) #text data\n",
        "print(x_train) #text data in tokens\n",
        "print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWSoUGpT_gs4"
      },
      "outputs": [],
      "source": [
        "df_reviews['Text'][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-P30ljQo_sXM"
      },
      "outputs": [],
      "source": [
        "df_reviews['Summary'][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLBZpckk_Raj"
      },
      "outputs": [],
      "source": [
        "print (summarizer(X_train[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-pXQeIzPCKt"
      },
      "outputs": [],
      "source": [
        "#for i in range(5):\n",
        "print (summarizer(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwAa5vvlWnf1"
      },
      "outputs": [],
      "source": [
        "df_reviews[\"Text\"][6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14uke2C3bGD4"
      },
      "outputs": [],
      "source": [
        "df_reviews[\"Summary\"][6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "nUGeci51KKtm"
      },
      "outputs": [],
      "source": [
        "vocab_size = 100\n",
        "src_txt_length = 1000\n",
        "sum_txt_length = 100\n",
        "# article input model\n",
        "inputs1 = Input(shape=(src_txt_length,))\n",
        "article1 = Embedding(vocab_size, 128)(inputs1)\n",
        "article2 = LSTM(128)(article1)\n",
        "article3 = RepeatVector(sum_txt_length)(article2)\n",
        "# summary input model\n",
        "inputs2 = Input(shape=(sum_txt_length,))\n",
        "summ1 = Embedding(vocab_size, 128)(inputs2)\n",
        "# decoder model\n",
        "decoder1 = concatenate([article3, summ1])\n",
        "decoder2 = LSTM(128)(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "# tie it together [article, summary] [word]\n",
        "DL_Model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "DL_Model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "dLN6Fe0uv1j4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "bbe602ee-27b1-42aa-807b-ede74a588fd7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-131-6be7ae39240e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDL_Model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"model_1\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 200) dtype=int32>]\n"
          ]
        }
      ],
      "source": [
        "DL_Model.fit(x_train,y_train,verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFAB5uiPzF16"
      },
      "outputs": [],
      "source": [
        "def define_models(n_input, n_output, n_units):\n",
        "# define training encoder\n",
        "  encoder_inputs = Input(shape=(None, n_input))\n",
        "  encoder = LSTM(n_units, return_state=True)\n",
        "  encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "  encoder_states = [state_h, state_c]\n",
        "# define training decoder\n",
        "  decoder_inputs = Input(shape=(None, n_output))\n",
        "  decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
        "  decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "  decoder_dense = Dense(n_output, activation='softmax')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "# define inference encoder\n",
        "  encoder_model = Model(encoder_inputs, encoder_states)\n",
        "# define inference decoder\n",
        "  decoder_state_input_h = Input(shape=(n_units,))\n",
        "  decoder_state_input_c = Input(shape=(n_units,))\n",
        "  decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "  decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs,  initial_state=decoder_states_inputs)\n",
        "  decoder_states = [state_h, state_c]\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "# return all models\n",
        "  return model, encoder_model, decoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyC9zja2ZoS7"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDIBQJJhb1tq"
      },
      "source": [
        "Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAg_GnVJbw9P"
      },
      "outputs": [],
      "source": [
        "model.fit"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}